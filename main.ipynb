{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GoogleTranslator import gtrans\n",
    "from tools import GetFar\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import jieba\n",
    "jieba.dt.cache_file = './jieba.cache.new'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  'conversation': “1\\t你好嗎\\t0.926\\n2\\t我很好喔\\n\",\n",
    "  'conversation_pin’: “1\\tni hat ma\\t0.926\\n2\\twou hen hao\\n\",\n",
    "  'question’: “1\\t請問…\\t0.899“,\n",
    "  'question_pin’: “1\\tchin woun …\\t0.899“,\n",
    "  'options’: “1\\t好\\t0.99\\n2\\t不好0.933”\n",
    "  'options_pin’: “1\\thao\\t0.99\\n2\\tbu hao0.933”\n",
    "}\n",
    "\n",
    "\n",
    "{ \n",
    "  'answer': 2, \n",
    "  'scores': [2, 1]\n",
    "}\n",
    "\n",
    "\n",
    "{  \n",
    "   \"passage\":\"A reusable launch system (RLS, or reusable launch vehicle, RLV) is a launch system which is capable of launching a payload into space more than once. This contrasts with expendable launch systems, where each launch vehicle is launched once and then discarded. No completely reusable orbital launch system has ever been created. Two partially reusable launch systems were developed, the Space Shuttle and Falcon 9. The Space Shuttle was partially reusable: the orbiter (which included the Space Shuttle main engines and the Orbital Maneuvering System engines), and the two solid rocket boosters were reused after several months of refitting work for each launch. The external tank was discarded after each flight.\",\n",
    "   \"question\":\"How many partially reusable launch systems were developed?\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from predict.predict import *\n",
    "\n",
    "def segment(sent):\n",
    "    seg_list = jieba.cut(sent)\n",
    "    return [seg for seg in seg_list if seg.strip()]\n",
    "\n",
    "\n",
    "def format_data(data):\n",
    "    return {\n",
    "        'conversation': data['conversation'].strip().split('\\n'),\n",
    "        'conversation_pin': data['conversation'].strip().split('\\n'),\n",
    "        'question': data['question'].strip(),\n",
    "        'question_pin': data['question'].strip(),\n",
    "        'options': data['options'].strip().split('\\n'),\n",
    "        'options_pin': data['options'].strip().split('\\n')\n",
    "    }\n",
    "\n",
    "\n",
    "def get_content(line):\n",
    "    return line.split('\\t')[1]\n",
    "\n",
    "\n",
    "def to_passage(conversation):\n",
    "    lines = [get_content(line) for line in conversation]\n",
    "    return '。'.join(lines)\n",
    "    \n",
    "\n",
    "def get_opt_pair(options):\n",
    "    return [(opt.split('\\t')[0], get_content(opt)) for opt in options]\n",
    "\n",
    "    \n",
    "def main_process(data):\n",
    "    data = format_data(data)\n",
    "    \n",
    "    # for mictsai\n",
    "    model_input = {\n",
    "        'passage': gtrans( to_passage(data['conversation']) ),\n",
    "        'question': gtrans( get_content(data['question']) ),\n",
    "        'options': [(i, gtrans(opt)) for i, opt in get_opt_pair(data['options'])]\n",
    "    }\n",
    "\n",
    "    pprint(model_input)\n",
    "    result = predict_json(model_input)\n",
    "    pprint(result)\n",
    "\n",
    "    cosine_pair = result['cosine'].items()\n",
    "    ans_idx = max(cosine_pair, key=itemgetter(1))[0]\n",
    "    scores = list(map(lambda pair: pair[1], cosine_pair))\n",
    "\n",
    "    # First method - 反向指標\n",
    "    # ans_idx, scores = GetFar(conver, opt_list)\n",
    "    \n",
    "    return ans_idx, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'options': [('1', 'I am full'), ('2', 'Have not eat yet')],\n",
      " 'passage': 'I just had enough to take a walk. are you full. Eat full',\n",
      " 'question': 'Is he full?'}\n",
      "Nb words kept : 9/9 (100.0%)\n",
      "Speed : 40.8 sentences/s (cpu mode, bsize=128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/GrandChallenge/predict/models.py:222: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  sentences[stidx:stidx + bsize]), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 15/15 (100.0%)\n",
      "Speed : 47.7 sentences/s (cpu mode, bsize=128)\n",
      "{'cosine': {'1': 0.57374698, '2': 0.51942182}, 'predict': 'Eat full'}\n",
      "('1', [0.57374698, 0.51942182])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "if __name__ == \"__main__\":\n",
    "    test = {\n",
    "      \"conversation\": \"1\\t我剛吃飽來散步\\t0.99\\n2\\t你吃飽了嗎\\t0.33\\n1\\t吃飽囉\\t0.87\",\n",
    "      \"conversation_pin\": \"1\\t我剛吃飽來散步\\t0.99\\n2\\t你吃飽了嗎\\t0.33\\n1\\t吃飽囉\\t0.87\",\n",
    "      \"question\": \"1\\t請問他吃飽了嗎\\t0.66\",\n",
    "      \"question_pin\": \"1\\t請問他吃飽了嗎\\t0.66\",\n",
    "      \"options\": \"1\\t吃飽了\\t0.99\\n2\\t還沒吃\\t0.99\",\n",
    "      \"options_pin\": \"1\\t吃飽了\\t0.99\\n2\\t還沒吃\\t0.99\"\n",
    "    }\n",
    "    print(main_process(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'options': [('1', 'I am full'), ('2', 'Have not eat yet')],\n",
      " 'passage': 'I just had enough to take a walk. are you full. Eat full',\n",
      " 'question': 'Is he full?'}\n",
      "{'cosine': {'1': [0.23068536818027496, 0.13727924227714539, 0.0, 0.0],\n",
      "            '2': [0.5109639167785645, 0.15044136345386505, 0.0, 0.0]},\n",
      " 'predict': 'Eat full'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def answer():\n",
    "    topic = request.get_json()\n",
    "    \n",
    "    if not topic: return jsonify({'status': 'wrong'})\n",
    "    \n",
    "    ans_idx, scores = main_process(topic)\n",
    "    \n",
    "    return jsonify({ \n",
    "        'answer': ans_idx,\n",
    "        'scores': scores\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=1314)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
